/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/__init__.py:17: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  import pkg_resources
/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
  import pynvml  # type: ignore[import]
2026-01-18 12:51:46,469	WARNING utils.py:460 -- Detecting docker specified CPUs. In previous versions of Ray, CPU detection in containers was incorrect. Please ensure that Ray has enough CPUs allocated. As a temporary workaround to revert to the prior behavior, set `RAY_USE_MULTIPROCESSING_CPU_COUNT=1` as an env var before starting Ray. Set the env var: `RAY_DISABLE_DOCKER_CPU_WARNING=1` to mute this warning.
2026-01-18 12:51:47,810	INFO worker.py:2012 -- Started a local Ray instance.
/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/ray/_private/worker.py:2051: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
  warnings.warn(
(pid=262022)/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/__init__.py:17: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(pid=262022)  import pkg_resources
(pid=262022)/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
(pid=262022)  import pynvml  # type: ignore[import]
(TaskRunner pid=262022){'actor_rollout_ref': {'actor': {'checkpoint': {'contents': ['model',
(TaskRunner pid=262022)                                                             'optimizer',
(TaskRunner pid=262022)                                                             'extra']},
(TaskRunner pid=262022)                                 'clip_ratio': 0.2,
(TaskRunner pid=262022)                                 'clip_ratio_c': 3.0,
(TaskRunner pid=262022)                                 'clip_ratio_high': 0.2,
(TaskRunner pid=262022)                                 'clip_ratio_low': 0.2,
(TaskRunner pid=262022)                                 'entropy_coeff': 0.001,
(TaskRunner pid=262022)                                 'fsdp_config': {'fsdp_size': -1,
(TaskRunner pid=262022)                                                 'offload_policy': False,
(TaskRunner pid=262022)                                                 'optimizer_offload': True,
(TaskRunner pid=262022)                                                 'param_offload': True,
(TaskRunner pid=262022)                                                 'reshard_after_forward': True,
(TaskRunner pid=262022)                                                 'wrap_policy': {'min_num_params': 0}},
(TaskRunner pid=262022)                                 'grad_clip': 1.0,
(TaskRunner pid=262022)                                 'kl_loss_coef': 0.001,
(TaskRunner pid=262022)                                 'kl_loss_type': 'low_var_kl',
(TaskRunner pid=262022)                                 'loss_agg_mode': 'seq-mean-token-mean',
(TaskRunner pid=262022)                                 'optim': {'lr': 1e-06,
(TaskRunner pid=262022)                                           'lr_warmup_steps': -1,
(TaskRunner pid=262022)                                           'lr_warmup_steps_ratio': 0.0,
(TaskRunner pid=262022)                                           'min_lr_ratio': 0.0,
(TaskRunner pid=262022)                                           'num_cycles': 0.5,
(TaskRunner pid=262022)                                           'total_training_steps': -1,
(TaskRunner pid=262022)                                           'warmup_style': 'constant',
(TaskRunner pid=262022)                                           'weight_decay': 0.01},
(TaskRunner pid=262022)                                 'ppo_epochs': 1,
(TaskRunner pid=262022)                                 'ppo_max_token_len_per_gpu': 12000,
(TaskRunner pid=262022)                                 'ppo_micro_batch_size': None,
(TaskRunner pid=262022)                                 'ppo_micro_batch_size_per_gpu': None,
(TaskRunner pid=262022)                                 'ppo_mini_batch_size': 32,
(TaskRunner pid=262022)                                 'shuffle': False,
(TaskRunner pid=262022)                                 'strategy': 'fsdp',
(TaskRunner pid=262022)                                 'ulysses_sequence_parallel_size': 1,
(TaskRunner pid=262022)                                 'use_dynamic_bsz': True,
(TaskRunner pid=262022)                                 'use_kl_loss': True,
(TaskRunner pid=262022)                                 'use_torch_compile': True},
(TaskRunner pid=262022)                       'hybrid_engine': True,
(TaskRunner pid=262022)                       'model': {'enable_activation_offload': False,
(TaskRunner pid=262022)                                 'enable_gradient_checkpointing': True,
(TaskRunner pid=262022)                                 'external_lib': None,
(TaskRunner pid=262022)                                 'override_config': {},
(TaskRunner pid=262022)                                 'path': '/wangbenyou-huanghejin/models/Qwen3-8B',
(TaskRunner pid=262022)                                 'trust_remote_code': False,
(TaskRunner pid=262022)                                 'use_fused_kernels': False,
(TaskRunner pid=262022)                                 'use_liger': False,
(TaskRunner pid=262022)                                 'use_remove_padding': True},
(TaskRunner pid=262022)                       'ref': {'fsdp_config': {'param_offload': True,
(TaskRunner pid=262022)                                               'reshard_after_forward': True,
(TaskRunner pid=262022)                                               'wrap_policy': {'min_num_params': 0}},
(TaskRunner pid=262022)                               'log_prob_max_token_len_per_gpu': 12000,
(TaskRunner pid=262022)                               'log_prob_micro_batch_size': None,
(TaskRunner pid=262022)                               'log_prob_micro_batch_size_per_gpu': None,
(TaskRunner pid=262022)                               'log_prob_use_dynamic_bsz': True,
(TaskRunner pid=262022)                               'strategy': 'fsdp',
(TaskRunner pid=262022)                               'ulysses_sequence_parallel_size': 1,
(TaskRunner pid=262022)                               'use_torch_compile': True},
(TaskRunner pid=262022)                       'rollout': {'chat_scheduler': None,
(TaskRunner pid=262022)                                   'disable_log_stats': True,
(TaskRunner pid=262022)                                   'do_sample': True,
(TaskRunner pid=262022)                                   'dtype': 'bfloat16',
(TaskRunner pid=262022)                                   'enable_chunked_prefill': True,
(TaskRunner pid=262022)                                   'enable_thinking': True,
(TaskRunner pid=262022)                                   'enforce_eager': True,
(TaskRunner pid=262022)                                   'engine_kwargs': {'sglang': {'attention_backend': None},
(TaskRunner pid=262022)                                                     'vllm': {'swap_space': None}},
(TaskRunner pid=262022)                                   'free_cache_engine': True,
(TaskRunner pid=262022)                                   'gpu_memory_utilization': 0.4,
(TaskRunner pid=262022)                                   'ignore_eos': False,
(TaskRunner pid=262022)                                   'load_format': 'dummy_dtensor',
(TaskRunner pid=262022)                                   'log_prob_max_token_len_per_gpu': 12000,
(TaskRunner pid=262022)                                   'log_prob_micro_batch_size': None,
(TaskRunner pid=262022)                                   'log_prob_micro_batch_size_per_gpu': None,
(TaskRunner pid=262022)                                   'log_prob_use_dynamic_bsz': True,
(TaskRunner pid=262022)                                   'max_model_len': None,
(TaskRunner pid=262022)                                   'max_num_batched_tokens': 32768,
(TaskRunner pid=262022)                                   'max_num_seqs': 1024,
(TaskRunner pid=262022)                                   'mode': 'sync',
(TaskRunner pid=262022)                                   'multi_turn': {'enable': False,
(TaskRunner pid=262022)                                                  'format': 'chatml',
(TaskRunner pid=262022)                                                  'max_turns': None,
(TaskRunner pid=262022)                                                  'tool_config_path': None},
(TaskRunner pid=262022)                                   'n': 16,
(TaskRunner pid=262022)                                   'name': 'vllm',
(TaskRunner pid=262022)                                   'prompt_length': 7000,
(TaskRunner pid=262022)                                   'response_length': 23000,
(TaskRunner pid=262022)                                   'temperature': 1.0,
(TaskRunner pid=262022)                                   'tensor_model_parallel_size': 1,
(TaskRunner pid=262022)                                   'top_k': -1,
(TaskRunner pid=262022)                                   'top_p': 1,
(TaskRunner pid=262022)                                   'use_fire_sampling': False,
(TaskRunner pid=262022)                                   'val_kwargs': {'do_sample': False,
(TaskRunner pid=262022)                                                  'n': 1,
(TaskRunner pid=262022)                                                  'temperature': 0,
(TaskRunner pid=262022)                                                  'top_k': -1,
(TaskRunner pid=262022)                                                  'top_p': 1.0}}},
(TaskRunner pid=262022) 'algorithm': {'adv_estimator': 'grpo',
(TaskRunner pid=262022)               'gamma': 1.0,
(TaskRunner pid=262022)               'kl_ctrl': {'horizon': 10000,
(TaskRunner pid=262022)                           'kl_coef': 0.001,
(TaskRunner pid=262022)                           'target_kl': 0.1,
(TaskRunner pid=262022)                           'type': 'fixed'},
(TaskRunner pid=262022)               'kl_penalty': 'kl',
(TaskRunner pid=262022)               'lam': 1.0,
(TaskRunner pid=262022)               'norm_adv_by_std_in_grpo': True,
(TaskRunner pid=262022)               'use_kl_in_reward': False},
(TaskRunner pid=262022) 'critic': {'checkpoint': {'contents': ['model', 'optimizer', 'extra']},
(TaskRunner pid=262022)            'cliprange_value': 0.5,
(TaskRunner pid=262022)            'forward_max_token_len_per_gpu': 32768,
(TaskRunner pid=262022)            'forward_micro_batch_size': None,
(TaskRunner pid=262022)            'forward_micro_batch_size_per_gpu': None,
(TaskRunner pid=262022)            'grad_clip': 'inf',
(TaskRunner pid=262022)            'loss_agg_mode': 'seq-mean-token-mean',
(TaskRunner pid=262022)            'model': {'enable_activation_offload': False,
(TaskRunner pid=262022)                      'enable_gradient_checkpointing': True,
(TaskRunner pid=262022)                      'external_lib': None,
(TaskRunner pid=262022)                      'fsdp_config': {'fsdp_size': -1,
(TaskRunner pid=262022)                                      'offload_policy': False,
(TaskRunner pid=262022)                                      'optimizer_offload': False,
(TaskRunner pid=262022)                                      'param_offload': False,
(TaskRunner pid=262022)                                      'reshard_after_forward': True,
(TaskRunner pid=262022)                                      'wrap_policy': {'min_num_params': 0}},
(TaskRunner pid=262022)                      'override_config': {},
(TaskRunner pid=262022)                      'path': '~/models/deepseek-llm-7b-chat',
(TaskRunner pid=262022)                      'tokenizer_path': '/wangbenyou-huanghejin/models/Qwen3-8B',
(TaskRunner pid=262022)                      'trust_remote_code': False,
(TaskRunner pid=262022)                      'use_remove_padding': False},
(TaskRunner pid=262022)            'optim': {'lr': 1e-05,
(TaskRunner pid=262022)                      'lr_warmup_steps_ratio': 0.0,
(TaskRunner pid=262022)                      'min_lr_ratio': None,
(TaskRunner pid=262022)                      'total_training_steps': -1,
(TaskRunner pid=262022)                      'warmup_style': 'constant',
(TaskRunner pid=262022)                      'weight_decay': 0.01},
(TaskRunner pid=262022)            'ppo_epochs': 1,
(TaskRunner pid=262022)            'ppo_max_token_len_per_gpu': 32768,
(TaskRunner pid=262022)            'ppo_micro_batch_size': None,
(TaskRunner pid=262022)            'ppo_micro_batch_size_per_gpu': None,
(TaskRunner pid=262022)            'ppo_mini_batch_size': 32,
(TaskRunner pid=262022)            'rollout_n': 16,
(TaskRunner pid=262022)            'shuffle': False,
(TaskRunner pid=262022)            'strategy': 'fsdp',
(TaskRunner pid=262022)            'ulysses_sequence_parallel_size': 1,
(TaskRunner pid=262022)            'use_dynamic_bsz': True},
(TaskRunner pid=262022) 'custom_reward_function': {'name': 'compute_process_KM',
(TaskRunner pid=262022)                            'path': 'Code/verl/utils/reward_score/tool.py'},
(TaskRunner pid=262022) 'data': {'custom_cls': {'name': None, 'path': None},
(TaskRunner pid=262022)          'enable_thinking': True,
(TaskRunner pid=262022)          'filter_overlong_prompts': True,
(TaskRunner pid=262022)          'filter_overlong_prompts_workers': 1,
(TaskRunner pid=262022)          'image_key': 'images',
(TaskRunner pid=262022)          'max_prompt_length': 7000,
(TaskRunner pid=262022)          'max_response_length': 23000,
(TaskRunner pid=262022)          'prompt_key': 'messages',
(TaskRunner pid=262022)          'return_full_prompt': False,
(TaskRunner pid=262022)          'return_raw_chat': False,
(TaskRunner pid=262022)          'return_raw_input_ids': False,
(TaskRunner pid=262022)          'reward_fn_key': 'data_source',
(TaskRunner pid=262022)          'shuffle': True,
(TaskRunner pid=262022)          'system_style': 'Qwen3',
(TaskRunner pid=262022)          'tokenizer': None,
(TaskRunner pid=262022)          'tools_key': 'tools',
(TaskRunner pid=262022)          'train_batch_size': 256,
(TaskRunner pid=262022)          'train_files': 'Data/train.parquet',
(TaskRunner pid=262022)          'truncation': 'error',
(TaskRunner pid=262022)          'val_batch_size': 256,
(TaskRunner pid=262022)          'val_files': 'Data/test.parquet',
(TaskRunner pid=262022)          'video_key': 'videos'},
(TaskRunner pid=262022) 'ray_init': {'num_cpus': None},
(TaskRunner pid=262022) 'reward_model': {'enable': False,
(TaskRunner pid=262022)                  'forward_max_token_len_per_gpu': 32768,
(TaskRunner pid=262022)                  'launch_reward_fn_async': False,
(TaskRunner pid=262022)                  'max_length': None,
(TaskRunner pid=262022)                  'micro_batch_size': None,
(TaskRunner pid=262022)                  'micro_batch_size_per_gpu': None,
(TaskRunner pid=262022)                  'model': {'external_lib': None,
(TaskRunner pid=262022)                            'fsdp_config': {'fsdp_size': -1,
(TaskRunner pid=262022)                                            'param_offload': False,
(TaskRunner pid=262022)                                            'reshard_after_forward': True,
(TaskRunner pid=262022)                                            'wrap_policy': {'min_num_params': 0}},
(TaskRunner pid=262022)                            'input_tokenizer': '/wangbenyou-huanghejin/models/Qwen3-8B',
(TaskRunner pid=262022)                            'path': '~/models/FsfairX-LLaMA3-RM-v0.1',
(TaskRunner pid=262022)                            'trust_remote_code': False,
(pid=gcs_server)[2026-01-18 12:52:12,343 E 258351 258351] (gcs_server) gcs_server.cc:302: Failed to establish connection to the event+metrics exporter agent. Events and metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[33m(raylet)[2026-01-18 12:52:16,965 E 258795 258795] (raylet) main.cc:975: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
(TaskRunner pid=262022)
Filtering prompts longer than 7000 tokens:   0%|          | 0/2215 [00:00<?, ? examples/s]
(pid=259091)[2026-01-18 12:52:18,540 E 259091 259423] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
[2026-01-18 12:52:19,185 E 257094 259056] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
(TaskRunner pid=262022)
Filtering prompts longer than 7000 tokens:  45%|████▌     | 1000/2215 [00:06<00:08, 149.37 examples/s]
(TaskRunner pid=262022)[2026-01-18 12:52:20,011 E 262022 262060] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14[32m [repeated 49x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
(TaskRunner pid=262022)
Filtering prompts longer than 7000 tokens:  90%|█████████ | 2000/2215 [00:13<00:01, 143.79 examples/s]
(TaskRunner pid=262022)
Filtering prompts longer than 7000 tokens: 100%|██████████| 2215/2215 [00:15<00:00, 141.03 examples/s]
Filtering prompts longer than 7000 tokens: 100%|██████████| 2215/2215 [00:15<00:00, 142.72 examples/s]
(TaskRunner pid=262022)
Filtering prompts longer than 7000 tokens:   0%|          | 0/200 [00:00<?, ? examples/s]
(TaskRunner pid=262022)
Filtering prompts longer than 7000 tokens: 100%|██████████| 200/200 [00:01<00:00, 141.83 examples/s]
Filtering prompts longer than 7000 tokens: 100%|██████████| 200/200 [00:01<00:00, 141.71 examples/s]
(TaskRunner pid=262022)DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
(TaskRunner pid=262022)WARNING:2026-01-18 12:52:37,368:Waiting for register center actor PHii4G_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
(pid=263179)/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/__init__.py:17: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(pid=263179)  import pkg_resources
(pid=263179)/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
(pid=263179)  import pynvml  # type: ignore[import]
(pid=263568)/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/__init__.py:17: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(pid=263568)  import pkg_resources
(pid=263567)/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/__init__.py:17: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(pid=263567)  import pkg_resources
(pid=263568)/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
(pid=263568)  import pynvml  # type: ignore[import]
(bundle_reservation_check_func pid=262802)[2026-01-18 12:53:07,040 E 262802 263092] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14
(pid=263569)/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/__init__.py:17: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
(pid=263569)  import pkg_resources
(pid=263569)/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.[32m [repeated 2x across cluster][0m
(pid=263569)  import pynvml  # type: ignore[import][32m [repeated 2x across cluster][0m
(WorkerGroupRegisterCenter pid=263437)[2026-01-18 12:53:19,181 E 263437 263480] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14[32m [repeated 2x across cluster][0m
(WorkerDict pid=263179)You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
(WorkerDict pid=263179)
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]
(pid=263569)[2026-01-18 12:53:20,228 E 263569 263729] core_worker_process.cc:825: Failed to establish connection to the metrics exporter agent. Metrics will not be exported. Exporter agent status: RpcError: Running out of retries to initialize the metrics agent. rpc_code: 14[32m [repeated 3x across cluster][0m
(WorkerDict pid=263179)
Loading checkpoint shards:  20%|██        | 1/5 [00:00<00:03,  1.16it/s]
(WorkerDict pid=263179)
Loading checkpoint shards:  80%|████████  | 4/5 [00:04<00:01,  1.02s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.22it/s]
(TaskRunner pid=262022)                            'use_fused_kernels': False,
(TaskRunner pid=262022)                            'use_remove_padding': False},
(TaskRunner pid=262022)                  'reward_manager': 'tool',
(TaskRunner pid=262022)                  'sandbox_fusion': {'max_concurrent': 64, 'url': None},
(TaskRunner pid=262022)                  'strategy': 'fsdp',
(TaskRunner pid=262022)                  'ulysses_sequence_parallel_size': 1,
(TaskRunner pid=262022)                  'use_dynamic_bsz': True},
(TaskRunner pid=262022) 'trainer': {'balance_batch': True,
(TaskRunner pid=262022)             'critic_warmup': 0,
(TaskRunner pid=262022)             'default_hdfs_dir': None,
(TaskRunner pid=262022)             'default_local_dir': 'checkpoints/MatchTIR/MatchTIR-8B',
(TaskRunner pid=262022)             'del_local_ckpt_after_load': False,
(TaskRunner pid=262022)             'experiment_name': 'MatchTIR-8B',
(TaskRunner pid=262022)             'log_val_generations': 0,
(TaskRunner pid=262022)             'logger': ['console', 'wandb'],
(TaskRunner pid=262022)             'max_actor_ckpt_to_keep': None,
(TaskRunner pid=262022)             'max_critic_ckpt_to_keep': None,
(TaskRunner pid=262022)             'n_gpus_per_node': 4,
(TaskRunner pid=262022)             'nnodes': 1,
(TaskRunner pid=262022)             'project_name': 'MatchTIR',
(TaskRunner pid=262022)             'ray_wait_register_center_timeout': 300,
(TaskRunner pid=262022)             'resume_from_path': None,
(TaskRunner pid=262022)             'resume_mode': 'auto',
(TaskRunner pid=262022)             'rollout_data_dir': 'rollout/MatchTIR/MatchTIR-8B',
(TaskRunner pid=262022)             'save_freq': 8,
(TaskRunner pid=262022)             'test_freq': 4,
(TaskRunner pid=262022)             'total_epochs': 3,
(TaskRunner pid=262022)             'total_training_steps': None,
(TaskRunner pid=262022)             'val_before_train': False,
(TaskRunner pid=262022)             'val_only': False,
(TaskRunner pid=262022)             'validation_data_dir': 'validation/MatchTIR/MatchTIR-8B'}}
(TaskRunner pid=262022)using customized reward function 'compute_process_KM' from 'Code/verl/utils/reward_score/tool.py'
(TaskRunner pid=262022)using customized reward function 'compute_process_KM' from 'Code/verl/utils/reward_score/tool.py'
(TaskRunner pid=262022)Using dataset class: RLHFDataset
(TaskRunner pid=262022)dataset len: 2215
(TaskRunner pid=262022)filter dataset len: 2215
(TaskRunner pid=262022)Using dataset class: RLHFDataset
(TaskRunner pid=262022)dataset len: 200
(TaskRunner pid=262022)filter dataset len: 200
(TaskRunner pid=262022)WARNING: val_batch_size is deprecated. Validation datasets are sent to inference engines as a whole batch, which will schedule the memory themselves.
(TaskRunner pid=262022)[validate_config] All configuration checks passed successfully!
(TaskRunner pid=262022)Size of train dataloader: 8, Size of val dataloader: 1
(TaskRunner pid=262022)Total training steps: 24
(TaskRunner pid=262022)colocated worker base class <class 'verl.single_controller.base.worker.Worker'>
(WorkerDict pid=263179)Model config after override: Qwen3Config {
(WorkerDict pid=263179)  "architectures": [
(WorkerDict pid=263179)    "Qwen3ForCausalLM"
(WorkerDict pid=263179)  ],
(WorkerDict pid=263179)  "attention_bias": false,
(WorkerDict pid=263179)  "attention_dropout": 0.0,
(WorkerDict pid=263179)  "eos_token_id": 151645,
(WorkerDict pid=263179)  "head_dim": 128,
(WorkerDict pid=263179)  "hidden_act": "silu",
(WorkerDict pid=263179)  "hidden_size": 4096,
(WorkerDict pid=263179)  "initializer_range": 0.02,
(WorkerDict pid=263179)  "intermediate_size": 12288,
(WorkerDict pid=263179)  "layer_types": [
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention"
(WorkerDict pid=263179)  ],
(WorkerDict pid=263179)  "max_position_embeddings": 40960,
(WorkerDict pid=263179)  "max_window_layers": 36,
(WorkerDict pid=263179)  "model_type": "qwen3",
(WorkerDict pid=263179)  "num_attention_heads": 32,
(WorkerDict pid=263179)  "num_hidden_layers": 36,
(WorkerDict pid=263179)  "num_key_value_heads": 8,
(WorkerDict pid=263179)  "pad_token_id": 151643,
(WorkerDict pid=263179)  "rms_norm_eps": 1e-06,
(WorkerDict pid=263179)  "rope_scaling": null,
(WorkerDict pid=263179)  "rope_theta": 1000000,
(WorkerDict pid=263179)  "sliding_window": null,
(WorkerDict pid=263179)  "tie_word_embeddings": false,
(WorkerDict pid=263179)  "torch_dtype": "bfloat16",
(WorkerDict pid=263179)  "transformers_version": "4.53.3",
(WorkerDict pid=263179)  "use_cache": true,
(WorkerDict pid=263179)  "use_sliding_window": false,
(WorkerDict pid=263179)  "vocab_size": 151936
(WorkerDict pid=263179)}
(WorkerDict pid=263179)
(WorkerDict pid=263179)Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
(WorkerDict pid=263179)Qwen3ForCausalLM contains 8.19B parameters
(WorkerDict pid=263179)wrap_policy: functools.partial(<function _or_policy at 0x7f9a9eb4c160>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f9a9eb4c040>, transformer_layer_cls={<class 'transformers.models.qwen3.modeling_qwen3.Qwen3DecoderLayer'>})])
(WorkerDict pid=263179)NCCL version 2.21.5+cuda12.4
(WorkerDict pid=263179)Actor use_remove_padding=True
(WorkerDict pid=263179)Actor use_fused_kernels=False
(WorkerDict pid=263179)/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.
(WorkerDict pid=263179)  import pynvml  # type: ignore[import]
(WorkerDict pid=263569)You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 3x across cluster][0m
(WorkerDict pid=263569)
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
(WorkerDict pid=263569)
Loading checkpoint shards:  60%|██████    | 3/5 [00:03<00:02,  1.06s/it][32m [repeated 11x across cluster][0m
(WorkerDict pid=263569)
Loading checkpoint shards:  80%|████████  | 4/5 [00:04<00:01,  1.02s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:04<00:00,  1.22it/s][32m [repeated 3x across cluster][0m
(WorkerDict pid=263568)Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
(WorkerDict pid=263569)/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.[32m [repeated 3x across cluster][0m
(WorkerDict pid=263569)  import pynvml  # type: ignore[import][32m [repeated 3x across cluster][0m
(WorkerDict pid=263568)
Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s][32m [repeated 4x across cluster][0m
(WorkerDict pid=263179)
Loading checkpoint shards:  40%|████      | 2/5 [00:15<00:24,  8.28s/it][32m [repeated 5x across cluster][0m
(WorkerDict pid=263179)Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen3ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 3x across cluster][0m
(WorkerDict pid=263179)
Loading checkpoint shards:  60%|██████    | 3/5 [00:26<00:18,  9.39s/it][32m [repeated 4x across cluster][0m
(WorkerDict pid=263179)
Loading checkpoint shards:  80%|████████  | 4/5 [00:35<00:09,  9.31s/it][32m [repeated 4x across cluster][0m
(WorkerDict pid=263179)
Loading checkpoint shards: 100%|██████████| 5/5 [00:39<00:00,  7.50s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:39<00:00,  7.90s/it]
(WorkerDict pid=263569)Monkey patch _flash_attention_forward in transformers.integrations.flash_attention[32m [repeated 3x across cluster][0m
(WorkerDict pid=263569)wrap_policy: functools.partial(<function _or_policy at 0x7faba13ac160>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7faba13ac040>, transformer_layer_cls={<class 'transformers.models.qwen3.modeling_qwen3.Qwen3DecoderLayer'>})])[32m [repeated 3x across cluster][0m
(WorkerDict pid=263179)Model config after override: Qwen3Config {
(WorkerDict pid=263179)  "architectures": [
(WorkerDict pid=263179)    "Qwen3ForCausalLM"
(WorkerDict pid=263179)  ],
(WorkerDict pid=263179)  "attention_bias": false,
(WorkerDict pid=263179)  "attention_dropout": 0.0,
(WorkerDict pid=263179)  "eos_token_id": 151645,
(WorkerDict pid=263179)  "head_dim": 128,
(WorkerDict pid=263179)  "hidden_act": "silu",
(WorkerDict pid=263179)  "hidden_size": 4096,
(WorkerDict pid=263179)  "initializer_range": 0.02,
(WorkerDict pid=263179)  "intermediate_size": 12288,
(WorkerDict pid=263179)  "layer_types": [
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention",
(WorkerDict pid=263179)    "full_attention"
(WorkerDict pid=263179)  ],
(WorkerDict pid=263179)  "max_position_embeddings": 40960,
(WorkerDict pid=263179)  "max_window_layers": 36,
(WorkerDict pid=263179)  "model_type": "qwen3",
(WorkerDict pid=263179)  "num_attention_heads": 32,
(WorkerDict pid=263179)  "num_hidden_layers": 36,
(WorkerDict pid=263179)  "num_key_value_heads": 8,
(WorkerDict pid=263179)  "pad_token_id": 151643,
(WorkerDict pid=263179)  "rms_norm_eps": 1e-06,
(WorkerDict pid=263179)  "rope_scaling": null,
(WorkerDict pid=263179)  "rope_theta": 1000000,
(WorkerDict pid=263179)  "sliding_window": null,
(WorkerDict pid=263179)  "tie_word_embeddings": false,
(WorkerDict pid=263179)  "torch_dtype": "bfloat16",
(WorkerDict pid=263179)  "transformers_version": "4.53.3",
(WorkerDict pid=263179)  "use_cache": true,
(WorkerDict pid=263179)  "use_sliding_window": false,
(WorkerDict pid=263179)  "vocab_size": 151936
(WorkerDict pid=263179)}
(WorkerDict pid=263179)
(WorkerDict pid=263569)Actor use_remove_padding=True[32m [repeated 3x across cluster][0m
(WorkerDict pid=263569)Actor use_fused_kernels=False[32m [repeated 3x across cluster][0m
(WorkerDict pid=263179)Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
(WorkerDict pid=263179)Qwen3ForCausalLM contains 8.19B parameters
(WorkerDict pid=263179)wrap_policy: functools.partial(<function _or_policy at 0x7f9a9eb4c160>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7f9a9eb4c040>, transformer_layer_cls={<class 'transformers.models.qwen3.modeling_qwen3.Qwen3DecoderLayer'>})])
(WorkerDict pid=263568)Monkey patch _flash_attention_forward in transformers.integrations.flash_attention
(WorkerDict pid=263568)Total steps: 24, num_warmup_steps: 0
(WorkerDict pid=263569)wrap_policy: functools.partial(<function _or_policy at 0x7faba13ac160>, policies=[functools.partial(<function transformer_auto_wrap_policy at 0x7faba13ac040>, transformer_layer_cls={<class 'transformers.models.qwen3.modeling_qwen3.Qwen3DecoderLayer'>})])[32m [repeated 3x across cluster][0m
(WorkerDict pid=263569)Monkey patch _flash_attention_forward in transformers.integrations.flash_attention[32m [repeated 2x across cluster][0m
(WorkerDict pid=263568)Actor use_remove_padding=True
(WorkerDict pid=263568)Actor use_fused_kernels=False
(WorkerDict pid=263179)Total steps: 24, num_warmup_steps: 0[32m [repeated 3x across cluster][0m
(WorkerDict pid=263179)WARNING 01-18 12:56:29 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
(WorkerDict pid=263567)Actor use_remove_padding=True[32m [repeated 3x across cluster][0m
(WorkerDict pid=263567)Actor use_fused_kernels=False[32m [repeated 3x across cluster][0m
(WorkerDict pid=263179)WARNING 01-18 12:56:32 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f98ec2fb040>
(WorkerDict pid=263179)WARNING 01-18 12:56:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
Error executing job with overrides: ['algorithm.adv_estimator=grpo', 'data.train_files=Data/train.parquet', 'data.val_files=Data/test.parquet', 'data.train_batch_size=256', 'data.val_batch_size=256', 'data.max_prompt_length=7000', 'data.max_response_length=23000', 'data.filter_overlong_prompts=True', 'data.truncation=error', 'data.prompt_key=messages', 'data.system_style=Qwen3', 'data.enable_thinking=True', 'actor_rollout_ref.model.path=/wangbenyou-huanghejin/models/Qwen3-8B', 'actor_rollout_ref.actor.optim.lr=1e-6', 'actor_rollout_ref.model.use_remove_padding=True', 'actor_rollout_ref.actor.ppo_mini_batch_size=32', 'actor_rollout_ref.actor.use_dynamic_bsz=True', 'actor_rollout_ref.actor.ppo_max_token_len_per_gpu=12000', 'actor_rollout_ref.actor.use_kl_loss=True', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.kl_loss_type=low_var_kl', 'actor_rollout_ref.actor.entropy_coeff=0.001', 'actor_rollout_ref.model.enable_gradient_checkpointing=True', 'actor_rollout_ref.actor.fsdp_config.param_offload=True', 'actor_rollout_ref.actor.fsdp_config.optimizer_offload=True', 'actor_rollout_ref.rollout.temperature=1.0', 'actor_rollout_ref.rollout.tensor_model_parallel_size=1', 'actor_rollout_ref.rollout.name=vllm', 'actor_rollout_ref.rollout.mode=sync', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.4', 'actor_rollout_ref.rollout.n=16', 'actor_rollout_ref.rollout.max_num_batched_tokens=32768', 'actor_rollout_ref.ref.fsdp_config.param_offload=True', 'reward_model.reward_manager=tool', 'custom_reward_function.path=Code/verl/utils/reward_score/tool.py', 'custom_reward_function.name=compute_process_KM', 'algorithm.use_kl_in_reward=False', 'trainer.critic_warmup=0', 'trainer.logger=[console,wandb]', 'trainer.project_name=MatchTIR', 'trainer.experiment_name=MatchTIR-8B', 'trainer.val_before_train=False', '+trainer.val_only=False', 'trainer.n_gpus_per_node=4', 'trainer.nnodes=1', 'trainer.save_freq=8', 'trainer.test_freq=4', 'trainer.total_epochs=3']
Traceback (most recent call last):
  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/trainer/main_ppo.py", line 30, in main
    run_ppo(config)
  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/trainer/main_ppo.py", line 42, in run_ppo
    ray.get(runner.run.remote(config))
  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/ray/_private/worker.py", line 2961, in get
    values, debugger_breakpoint = worker.get_objects(
  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/ray/_private/worker.py", line 1026, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ValueError): ray::TaskRunner.run()[39m (pid=262022, ip=100.66.129.26, actor_id=aa14edee2901e4d96a210d9a01000000, repr=<main_ppo.TaskRunner object at 0x7ee121427640>)
  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/trainer/main_ppo.py", line 149, in run
    trainer.init_workers()
  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/trainer/ppo/ray_trainer.py", line 759, in init_workers
    self.actor_rollout_wg.init_model()
  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/single_controller/ray/base.py", line 49, in func
    output = ray.get(output)
ray.exceptions.RayTaskError(ValueError): ray::WorkerDict.actor_rollout_init_model()[39m (pid=263567, ip=100.66.129.26, actor_id=905baa9231913d41c80c4fb801000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f1d1cda11b0>)
  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/single_controller/ray/base.py", line 634, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/single_controller/base/decorator.py", line 534, in inner
    return func(*args, **kwargs)
  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/workers/fsdp_workers.py", line 546, in init_model
    self.rollout, self.rollout_sharding_manager = self._build_rollout(trust_remote_code=self.config.model.get("trust_remote_code", False))
  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/workers/fsdp_workers.py", line 394, in _build_rollout
    rollout = vllm_rollout_cls(
  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py", line 239, in __init__
    self.inference_engine = LLM(
  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/utils.py", line 1161, in inner
    return fn(*args, **kwargs)
  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 138, in from_engine_args
    return cls(vllm_config=vllm_config,
  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 92, in __init__
    self.engine_core = EngineCoreClient.make_client(
  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 75, in make_client
    return InprocClient(vllm_config, executor_class, log_stats)
  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 198, in __init__
    self.engine_core = EngineCore(*args, **kwargs)
  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 71, in __init__
    self._initialize_kv_caches(vllm_config)
  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 133, in _initialize_kv_caches
    kv_cache_configs = [
  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 134, in <listcomp>
    get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py", line 699, in get_kv_cache_config
    check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py", line 545, in check_enough_kv_cache_memory
    raise ValueError(
ValueError: To serve at least one request with the models's max seq len (30000), (4.12 GiB KV cache is needed, which is larger than the available KV cache memory (1.24 GiB). Based on the available memory,  Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
(TaskRunner pid=262022)Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::WorkerDict.actor_rollout_init_model()[39m (pid=263569, ip=100.66.129.26, actor_id=661b5e71c1251fac6fb9922301000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fab4e0a84c0>)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/single_controller/ray/base.py", line 634, in func
(TaskRunner pid=262022)    return getattr(self.worker_dict[key], name)(*args, **kwargs)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/single_controller/base/decorator.py", line 534, in inner
(TaskRunner pid=262022)    return func(*args, **kwargs)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/workers/fsdp_workers.py", line 546, in init_model
(TaskRunner pid=262022)    self.rollout, self.rollout_sharding_manager = self._build_rollout(trust_remote_code=self.config.model.get("trust_remote_code", False))
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/workers/fsdp_workers.py", line 394, in _build_rollout
(TaskRunner pid=262022)    rollout = vllm_rollout_cls(
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py", line 239, in __init__
(TaskRunner pid=262022)    self.inference_engine = LLM(
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/utils.py", line 1161, in inner
(TaskRunner pid=262022)    return fn(*args, **kwargs)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
(TaskRunner pid=262022)    self.llm_engine = LLMEngine.from_engine_args(
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 138, in from_engine_args
(TaskRunner pid=262022)    return cls(vllm_config=vllm_config,
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 92, in __init__
(TaskRunner pid=262022)    self.engine_core = EngineCoreClient.make_client(
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 75, in make_client
(TaskRunner pid=262022)    return InprocClient(vllm_config, executor_class, log_stats)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 198, in __init__
(TaskRunner pid=262022)    self.engine_core = EngineCore(*args, **kwargs)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 71, in __init__
(TaskRunner pid=262022)    self._initialize_kv_caches(vllm_config)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 133, in _initialize_kv_caches
(TaskRunner pid=262022)    kv_cache_configs = [
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 134, in <listcomp>
(TaskRunner pid=262022)    get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py", line 699, in get_kv_cache_config
(TaskRunner pid=262022)    check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py", line 545, in check_enough_kv_cache_memory
(TaskRunner pid=262022)    raise ValueError(
(TaskRunner pid=262022)ValueError: To serve at least one request with the models's max seq len (30000), (4.12 GiB KV cache is needed, which is larger than the available KV cache memory (1.24 GiB). Based on the available memory,  Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
(TaskRunner pid=262022)Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::WorkerDict.actor_rollout_init_model()[39m (pid=263568, ip=100.66.129.26, actor_id=8168a24cdd134219c075932401000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f5491894fd0>)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/single_controller/ray/base.py", line 634, in func
(TaskRunner pid=262022)    return getattr(self.worker_dict[key], name)(*args, **kwargs)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/single_controller/base/decorator.py", line 534, in inner
(TaskRunner pid=262022)    return func(*args, **kwargs)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/workers/fsdp_workers.py", line 546, in init_model
(TaskRunner pid=262022)    self.rollout, self.rollout_sharding_manager = self._build_rollout(trust_remote_code=self.config.model.get("trust_remote_code", False))
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/workers/fsdp_workers.py", line 394, in _build_rollout
(TaskRunner pid=262022)    rollout = vllm_rollout_cls(
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py", line 239, in __init__
(TaskRunner pid=262022)    self.inference_engine = LLM(
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/utils.py", line 1161, in inner
(TaskRunner pid=262022)    return fn(*args, **kwargs)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
(TaskRunner pid=262022)    self.llm_engine = LLMEngine.from_engine_args(
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 138, in from_engine_args
(TaskRunner pid=262022)    return cls(vllm_config=vllm_config,
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 92, in __init__
(TaskRunner pid=262022)    self.engine_core = EngineCoreClient.make_client(
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 75, in make_client
(TaskRunner pid=262022)    return InprocClient(vllm_config, executor_class, log_stats)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 198, in __init__
(TaskRunner pid=262022)    self.engine_core = EngineCore(*args, **kwargs)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 71, in __init__
(TaskRunner pid=262022)    self._initialize_kv_caches(vllm_config)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 133, in _initialize_kv_caches
(TaskRunner pid=262022)    kv_cache_configs = [
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 134, in <listcomp>
(TaskRunner pid=262022)    get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py", line 699, in get_kv_cache_config
(TaskRunner pid=262022)    check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py", line 545, in check_enough_kv_cache_memory
(TaskRunner pid=262022)    raise ValueError(
(TaskRunner pid=262022)ValueError: To serve at least one request with the models's max seq len (30000), (4.12 GiB KV cache is needed, which is larger than the available KV cache memory (1.24 GiB). Based on the available memory,  Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
(TaskRunner pid=262022)Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::WorkerDict.actor_rollout_init_model()[39m (pid=263179, ip=100.66.129.26, actor_id=f458112058565124512974c701000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f9a52a4c4c0>)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/single_controller/ray/base.py", line 634, in func
(TaskRunner pid=262022)    return getattr(self.worker_dict[key], name)(*args, **kwargs)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/single_controller/base/decorator.py", line 534, in inner
(TaskRunner pid=262022)    return func(*args, **kwargs)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/workers/fsdp_workers.py", line 546, in init_model
(TaskRunner pid=262022)    self.rollout, self.rollout_sharding_manager = self._build_rollout(trust_remote_code=self.config.model.get("trust_remote_code", False))
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/workers/fsdp_workers.py", line 394, in _build_rollout
(TaskRunner pid=262022)    rollout = vllm_rollout_cls(
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/workspace/rl/MatchTIR/Code/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py", line 239, in __init__
(TaskRunner pid=262022)    self.inference_engine = LLM(
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/utils.py", line 1161, in inner
(TaskRunner pid=262022)    return fn(*args, **kwargs)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/entrypoints/llm.py", line 247, in __init__
(TaskRunner pid=262022)    self.llm_engine = LLMEngine.from_engine_args(
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 138, in from_engine_args
(TaskRunner pid=262022)    return cls(vllm_config=vllm_config,
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py", line 92, in __init__
(TaskRunner pid=262022)    self.engine_core = EngineCoreClient.make_client(
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 75, in make_client
(TaskRunner pid=262022)    return InprocClient(vllm_config, executor_class, log_stats)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core_client.py", line 198, in __init__
(TaskRunner pid=262022)    self.engine_core = EngineCore(*args, **kwargs)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 71, in __init__
(TaskRunner pid=262022)    self._initialize_kv_caches(vllm_config)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 133, in _initialize_kv_caches
(TaskRunner pid=262022)    kv_cache_configs = [
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/engine/core.py", line 134, in <listcomp>
(TaskRunner pid=262022)    get_kv_cache_config(vllm_config, kv_cache_spec_one_worker,
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py", line 699, in get_kv_cache_config
(TaskRunner pid=262022)    check_enough_kv_cache_memory(vllm_config, kv_cache_spec, available_memory)
(TaskRunner pid=262022)  File "/wangbenyou-huanghejin/miniconda3/envs/MatchTIR/lib/python3.10/site-packages/vllm/v1/core/kv_cache_utils.py", line 545, in check_enough_kv_cache_memory
(TaskRunner pid=262022)    raise ValueError(
(TaskRunner pid=262022)ValueError: To serve at least one request with the models's max seq len (30000), (4.12 GiB KV cache is needed, which is larger than the available KV cache memory (1.24 GiB). Based on the available memory,  Try increasing `gpu_memory_utilization` or decreasing `max_model_len` when initializing the engine.
(WorkerDict pid=263569)
Loading checkpoint shards:  80%|████████  | 4/5 [00:35<00:09,  9.32s/it][32m [repeated 3x across cluster][0m
(WorkerDict pid=263569)
Loading checkpoint shards: 100%|██████████| 5/5 [00:39<00:00,  7.52s/it]
Loading checkpoint shards: 100%|██████████| 5/5 [00:39<00:00,  7.96s/it][32m [repeated 3x across cluster][0m
(WorkerDict pid=263569)WARNING 01-18 12:56:29 [cuda.py:93] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used[32m [repeated 3x across cluster][0m
(WorkerDict pid=263569)WARNING 01-18 12:56:32 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa9d81784f0>[32m [repeated 3x across cluster][0m
(WorkerDict pid=263569)WARNING 01-18 12:56:33 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.[32m [repeated 3x across cluster][0m
